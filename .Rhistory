model.diag.metrics <- augment(mod_lin)
library(broom)
model.diag.metrics <- augment(mod_lin)
head(model.diag.metrics)
model.diag.metrics %>% top_n(10, wt = .cooksd)
out <- model.diag.metrics %>% top_n(10, wt = .cooksd)
out$log.bmi.
subset(df, log(bmi) != out$log.bmi.)
df
subset(df, log(bmi) != out$log.bmi.)
df$log_bmi <- log(df$bmi)
subset(df, log_bmi != out$log.bmi.)
out$log.bmi.
df[-out$log.bmi.,]
-out$log.bmi
out$log.bmi.
df$log_bmi[df$log_bmi != out$log.bmi.]
df$log_bmi[df$log_bmi != out$log.bmi.]
out
log(df$bmi)
out
df <- read.csv("all_vars_df.csv")
df
model.diag.metrics <- augment(mod_lin)
head(model.diag.metrics)
out <- model.diag.metrics %>% top_n(10, wt = .cooksd)
df$log_bmi <- log(df$bmi)
df$log_bmi[df$log_bmi != out$log.bmi.]
df
df$log_bmi <- log(df$bmi)
out$log.bmi.
log(df$bmi)
df$log_bmi[df$log_bmi != out$log.bmi.,]
df$log_bmi[-out$log.bmi.]
df$log_bmi[-out$log.bmi.,]
df$log_bmi[-out$log.bmi.]
out$log.bmi.
df$log_bmi[-c(out$log.bmi.)]
df$log_bmi
df$log_bmi[-c(out$log.bmi.)]
out
ks.test(residuals(mod_lin), "rnorm") #Ipotesi di normalit? rifiutata
kurtosis.norm.test(mod_lin$residuals, nrepl=2000)  #H0 normalit??, p-value < livello alpha: non norm.
library(normtest)
install.packages("normtest")
library(normtest)
kurtosis.norm.test(mod_lin$residuals, nrepl=2000)  #H0 normalit??, p-value < livello alpha: non norm.
yplot <- function(x, nbreaks=10) {
z <- x
hist(z, breaks=nbreaks, freq=FALSE,
xlab="Price_charges",
main="Distribuzione della variabile dipendente", col = "grey")
rug(jitter(z), col="brown")
curve(dnorm(x, mean=mean(z), sd=sd(z)),
add=TRUE, col="red", lwd=2)
lines(density(z)$x, density(z)$y,
col="black", lwd=2, lty = 2)
legend("topright",
legend = c( "Normal Curve", "Kernel Density Curve"),
lty=1:2, col=c("red","black"), cex=.7)
}
#Densità + Forma
yplot(df$bmi)
plot(df$bmi)
text(df$bmi, labels=index, cex= 0.7)
plot(df$bmi)
text(df$bmi, labels=index, cex= 0.7)
plot(df$bmi)
text(df$bmi, labels=Index, cex= 0.7)
text(df$bmi, labels=df$id, cex= 0.7)
plot(df$bmi)
text(df$bmi, labels=df$id, cex= 0.7)
no_out <- df
id_out <- c(3013,3014,4009,3056,3046,3022,3018z)
id_out <- c(3013,3014,4009,3056,3046,3022,3018)
no_out$id
no_out$id[no_out$id != id_out]
no_out$id
no_out
no_out$id[no_out$id == id_out]
no_out$id[no_out$id == id_out]
subset(no_out, id != id_out)
df
no_out <- df
id_out <- c(3013,3014,4009,3056,3046,3022,3018)
subset(no_out, id != id_out)
no_out[no_out$id %in% id_out,]
no_out[no_out$id %notin% id_out,]
no_out[-no_out$id %in% id_out,]
no_out[!(no_out$id %in% id_out),]
no_out <- no_out[!(no_out$id %in% id_out),]
log(df$bmi)
qqnorm(no_out$bmi, pch = 1, frame = FALSE, col = "tomato")
qqline(no_out$bmi, col = "steelblue", lwd = 2)
qqnorm(log(no_out$bmi), pch = 1, frame = FALSE, col = "tomato") #sembrerebbe che il log permetta di schiacciare la coda
qqline(log(no_out$bmi), col = "steelblue", lwd = 2)
shapiro.test(no_out$bmi) #H0: normalità, si rigetta l'ipotesi nulla
shapiro.test(log(no_out$bmi)) #Il valore W si avvicina di più a 1
mod_lin <- lm(bmi ~  PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa + age + sex1m2f, no_out)
summary(mod_lin)
anova(mod_lin)
summary(mod_lin)
white.test <- function(lmod,data=df){
u2 <- lmod$residuals^2
y <- fitted(lmod)
Ru2 <- summary(lm(u2 ~ y + I(y^2)))$r.squared
LM <- nrow(data)*Ru2
p.value <- 1-pchisq(LM, 2)
data.frame("Test statistic"=LM,"P value"=p.value)
}
plot(1:nrow(no_out),resid(mod_lin),xlab="Observation Index",ylab="Residui",pch=19) #Graficamente non si vede bene la correlazine dei residui. E' piu utile in questo caso il test di dwatson
pander(dwtest(mod_lin),big.mark=",") #essendo la statistica d compresa tra 1 e 3 possiamo affermare che non c'? autocorrelazione seriale di primo ordine tra i residui (ricorda che per d<1 c'? autocorrelazione positiva mentre per d>3 c'? autocorrelazione negativa) In ogni caso non mi aspetto un'autocorrelazione dei residui in quanto non ? una serie temporale (e anche se ci fosse autocorrelazione potrei decidere di non correggerla proprio perch? non ? una serie temporale)
ks.test(residuals(mod_lin), "rnorm") #Ipotesi di normalit? rifiutata
kurtosis.norm.test(mod_lin$residuals, nrepl=2000)  #H0 normalit??, p-value < livello alpha: non norm.
stdres <- rstandard(mod_lin) #residui standardizzati
probDist <- pnorm(stdres) #probabilit?? dei residui standardizzati
plot(ppoints(length(stdres)), sort(probDist), main = "PP-Plot", xlab = "Observed Probability", ylab = "Expected Probability")
abline(0,1)
plot(mod_lin,which=2)
shapiro.test(mod_lin$residuals) #Ipotesi di normalit? rifiutata
plot(df$bmi)
text(df$bmi, labels=df$id, cex= 0.7)
abline(y = 30)
hline(y = 30)
line(y = 30)
plot(df$bmi)
line(y = 30)
line(x = 0,y = 30)
plot(df$bmi)
line(x = 0,y = 30)
line(x = c(1:100),y = 30)
plot(df$bmi)
abline(h=30, col="blue")
text(df$bmi, labels=df$id, cex= 0.7)
plot(df$bmi)
abline(h=34, col="blue")
text(df$bmi, labels=df$id, cex= 0.7)
no_out <- df
id_out <- c(4018,4009,3056,3046,3022,3018)
no_out <- no_out[!(no_out$id %in% id_out),]
qqnorm(no_out$bmi, pch = 1, frame = FALSE, col = "tomato")
qqline(no_out$bmi, col = "steelblue", lwd = 2)
qqnorm(log(no_out$bmi), pch = 1, frame = FALSE, col = "tomato")
qqline(log(no_out$bmi), col = "steelblue", lwd = 2)
shapiro.test(no_out$bmi) #H0: normalità, si accetta l'ipotesi di normalità dopo aver eliminato 7 outliers ()
shapiro.test(log(no_out$bmi)) #In questo caso il log peggiora
wilcox.test(df$bmi~df$sex1m2f,  conf.int = T, paired = F) #non esiste una differenza significativa tra le distribuzioni delle due popolazioni legate al sesso. Si accetta l'ipotesi nulla ad un livello di confidenza del 95%.
mod_lin <- lm(bmi ~  PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa + age + sex1m2f, no_out)
summary(mod_lin)
anova(mod_lin)
white.test <- function(lmod,data=df){
u2 <- lmod$residuals^2
y <- fitted(lmod)
Ru2 <- summary(lm(u2 ~ y + I(y^2)))$r.squared
LM <- nrow(data)*Ru2
p.value <- 1-pchisq(LM, 2)
data.frame("Test statistic"=LM,"P value"=p.value)
}
white.test(mod_lin)
library(normtest)
kurtosis.norm.test(mod_lin$residuals, nrepl=2000)  #H0 normalit??, p-value < livello alpha: non norm.
#In questo caso si rifiuta l'ipotesi di normalit??
shapiro.test(mod_lin$residuals) #Ipotesi di normalit? rifiutata
stdres <- rstandard(mod_lin) #residui standardizzati
probDist <- pnorm(stdres) #probabilit?? dei residui standardizzati
plot(ppoints(length(stdres)), sort(probDist), main = "PP-Plot", xlab = "Observed Probability", ylab = "Expected Probability")
abline(0,1)
abline(h=36, col="blue")
plot(df$bmi)
abline(h=36, col="blue")
text(df$bmi, labels=df$id, cex= 0.7)
plot(df$bmi)
abline(h=36, col="blue")
abline(h=10, col = "blu")
plot(df$bmi)
abline(h=36, col="blue")
abline(h=10, col = "blue")
text(df$bmi, labels=df$id, cex= 0.7)
white.test <- function(lmod,data=no_out){
u2 <- lmod$residuals^2
y <- fitted(lmod)
Ru2 <- summary(lm(u2 ~ y + I(y^2)))$r.squared
LM <- nrow(data)*Ru2
p.value <- 1-pchisq(LM, 2)
data.frame("Test statistic"=LM,"P value"=p.value)
}
white.test(mod_lin)
plot(1:nrow(no_out),resid(mod_lin),xlab="Observation Index",ylab="Residui",pch=19) #Graficamente non si vede bene la correlazine dei residui. E' piu utile in questo caso il test di dwatson
abline(h=0,col=2,lwd=3,lty=2)
kurtosis.norm.test(mod_lin$residuals, nrepl=2000)  #H0 normalit??, p-value < livello alpha: non norm.
shapiro.test(mod_lin$residuals) #Ipotesi di normalit? rifiutata
df2
df2 <- df
lista <- c("id","visitdate","heightcm","weightkg","vdate","heightm")
df2[,lista] <- list(NULL)
df2 <-  no_out
lista <- c("id","visitdate","heightcm","weightkg","vdate","heightm")
df2
df2[,lista] <- list(NULL)
df2
df2 <-  no_out
lista <- c("id","visitdate","heightcm","weightkg","vdate","heightm","zbmius","zbmicatus")
df2[,lista] <- list(NULL)
df2
# Fit the full model
full.model <- lm(bmi ~ ., df2)
# Stepwise regression model
step.modelb <- stepAIC(full.model, direction = "backward",
trace = FALSE)
# Fit the full model
full.model <- lm(bmi ~ PC1_taxa + PC2_taxa + PC3_taxa, df2)
# Stepwise regression model
step.modelb <- stepAIC(full.model, direction = "backward",
trace = FALSE)
step.modelb
#Oppure:
# Train the model
train.control <- trainControl(method = "cv", number = 10) #procedura di crossvalidation
train.control <- trainControl(method = "cv", number = 10) #procedura di crossvalidation
step.modelB <- train(bmi ~PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa + age + sex1m2f, data = no_out,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:8), #da 1 a 3 variabili
trControl = train.control
)
step.modelB
train.control <- trainControl(method = "cv", number = 10) #procedura di crossvalidation
step.modelB <- train(bmi ~PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa + age + sex1m2f, data = no_out,
method = "leapForward",
tuneGrid = data.frame(nvmax = 1:8), #da 1 a 3 variabili
trControl = train.control
)
step.modelB
step.modelB <- train(bmi ~PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f, data = no_out,
method = "leapForward",
tuneGrid = data.frame(nvmax = 1:8), #da 1 a 3 variabili
trControl = train.control
)
step.modelB
train.control <- trainControl(method = "cv", number = 10) #procedura di crossvalidation
step.modelB <- train(bmi ~PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f, data = no_out,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:8), #da 1 a 3 variabili
trControl = train.control
)
step.modelB
# Fit the full model
full.model <- lm(bmi ~PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f, data = no_out)
# Stepwise regression model
step.modelb <- stepAIC(full.model, direction = "backward",
trace = FALSE)
step.modelf <- stepAIC(full.model, direction = "forward",
trace = FALSE)
step.modelb
step.modelb
summary(step.modelb)
step.modelf <- stepAIC(full.model, direction = "forward",
trace = FALSE)
summary(step.modelf)
# Fit the full model
full.model <- lm(bmi ~., data = no_out)
full.model
bmi ~., data = no_out
no_out
df2 <-  no_out
lista <- c("id","visitdate","heightcm","weightkg","vdate","heightm","zbmius","zbmicatus")
df2[,lista] <- list(NULL)
df2
# Fit the full model
full.model <- lm(bmi ~., data = df2)
lm(bmi ~., data = df2)
df2
df2 <-  no_out
lista <- c("id","visitdate","heightcm","weightkg","vdate","heightm","zbmius","zbmicatus","birthdate","bdate")
df2[,lista] <- list(NULL)
df2
# Fit the full model
full.model <- lm(bmi ~., data = df2)
full.model
# Fit the full model
full.model <- lm(bmi ~ PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f +
PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa, data = df2)
full.model
# Stepwise regression model
step.modelb <- stepAIC(full.model, direction = "backward",
trace = FALSE)
step.modelf <- stepAIC(full.model, direction = "forward",
trace = FALSE)
step.modelb
summary(step.modelb)
summary(step.modelf)
step.modelf <- stepAIC(full.model, direction = "forward",
trace = FALSE)
step.modelf
summary(step.modelf)
#Analisi pca batteri, var. demografiche, bmi
var_num <- c("bmi", "PC1_taxa","PC2_taxa","PC3_taxa","PC4_taxa","PC5_taxa","PC6_taxa","age","sex1m2f", "PC1_nut","PC2_nut","PC3_nut","PC4_nut","PC5_nut")
coor <- cor(df[,var_numeric])
library(corrplot)
coor <- cor(df[,var_numeric])
var_num <- c("bmi", "PC1_taxa","PC2_taxa","PC3_taxa","PC4_taxa","PC5_taxa","PC6_taxa","age","sex1m2f", "PC1_nut","PC2_nut","PC3_nut","PC4_nut","PC5_nut")
library(corrplot)
coor <- cor(df[,var_num])
corrplot(coor, method="circle")
coor
corrplot(coor, method="circle")
coor <- cor(df[,var_num])
coor
train.control <- trainControl(method = "cv", number = 10) #procedura di crossvalidation
step.modelB <- train(bmi ~PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f +
PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa, data = df2,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:8), #da 1 a 3 variabili
trControl = train.control
)
step.modelB
step.modelB <- train(bmi ~PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f +
PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa, data = df2,
method = "leapforward",
tuneGrid = data.frame(nvmax = 1:8), #da 1 a 3 variabili
trControl = train.control
)
train.control <- trainControl(method = "cv", number = 10) #procedura di crossvalidation
step.modelB <- train(bmi ~PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f +
PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa, data = df2,
method = "leapForward",
tuneGrid = data.frame(nvmax = 1:8), #da 1 a 3 variabili
trControl = train.control
)
step.modelB
train.control <- trainControl(method = "cv", number = 10) #procedura di crossvalidation
step.modelB <- train(bmi ~PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f +
PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa, data = df2,
method = "leapForward",
tuneGrid = data.frame(nvmax = 1:13), #da 1 a 3 variabili
trControl = train.control
)
step.modelB
train.control <- trainControl(method = "cv", number = 10) #procedura di crossvalidation
step.modelB <- train(bmi ~PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f +
PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa, data = df2,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:13), #da 1 a 3 variabili
trControl = train.control
)
step.modelB
install.packages("olsrr")
library("olsrr")
install.packages("pkgInfo")
library("olsrr")
devtools::install_github("rsquaredacademy/olsrr")
library("olsrr")
devtools::install_github("rsquaredacademy/olsrr")
ls_plot_cooksd_chart <- function(model, print_plot = TRUE) {
check_model(model)
obs <- NULL
ckd <- NULL
txt <- NULL
cd  <- NULL
k <- ols_prep_cdplot_data(model)
d <- ols_prep_outlier_obs(k)
f <- ols_prep_cdplot_outliers(k)
p <- ggplot(d, aes(x = obs, y = cd, label = txt, ymin = min(cd), ymax = cd)) +
geom_linerange(colour = "blue") + geom_point(shape = 1, colour = "blue") +
geom_hline(yintercept = k$ts, colour = "red") + xlab("Observation") +
ylab("Cook's D") + ggtitle("Cook's D Chart") +
geom_text(vjust = -1, size = 3, family = "serif", fontface = "italic",
colour = "darkred", na.rm = TRUE) +
annotate(
"text", x = Inf, y = Inf, hjust = 1.2, vjust = 2,
family = "serif", fontface = "italic", colour = "darkred",
label = paste("Threshold:", round(k$ts, 3))
)
if (print_plot) {
suppressWarnings(print(p))
} else {
return(list(plot = p, outliers = f, threshold = k$ts))
}
}
ols_cooksd_chart <- function(model) {
.Deprecated("ols_plot_cooksd_chart()")
}
ols_cooksd_chart(mod_lin)
ls_plot_cooksd_chart <- function(model, print_plot = TRUE) {
check_model(model)
obs <- NULL
ckd <- NULL
txt <- NULL
cd  <- NULL
k <- ols_prep_cdplot_data(model)
d <- ols_prep_outlier_obs(k)
f <- ols_prep_cdplot_outliers(k)
p <- ggplot(d, aes(x = obs, y = cd, label = txt, ymin = min(cd), ymax = cd)) +
geom_linerange(colour = "blue") + geom_point(shape = 1, colour = "blue") +
geom_hline(yintercept = k$ts, colour = "red") + xlab("Observation") +
ylab("Cook's D") + ggtitle("Cook's D Chart") +
geom_text(vjust = -1, size = 3, family = "serif", fontface = "italic",
colour = "darkred", na.rm = TRUE) +
annotate(
"text", x = Inf, y = Inf, hjust = 1.2, vjust = 2,
family = "serif", fontface = "italic", colour = "darkred",
label = paste("Threshold:", round(k$ts, 3))
)
if (print_plot) {
suppressWarnings(print(p))
} else {
return(list(plot = p, outliers = f, threshold = k$ts))
}
}
ls_cooksd_chart(mod_lin)
ls_plot_cooksd_chart(mod_lin)
library("olsrr")
ols_plot_cooksd_chart(mod_lin)
olsrr::ols_plot_cooksd_chart(mod_lin)
install.packages("pkgInfo")
devtools::install_github("rsquaredacademy/pkginfo")
library("olsrr")
library(pkgbuild)
library(pkgconfig)
library(pkgload)
devtools::install_github("rsquaredacademy/pkginfo")
library(pkginfo)
library("olsrr")
library(olsrr)
library(olsrr)
remove.packages("olsrr", lib="~/R/win-library/3.6")
install.packages(olsrr)
install.packages("olsrr")
install.packages("olsrr")
library(olsrr)
install.packages("packageDiff")
library(olsrr)
pkgInfo(olsrr)
library(packageDiff)
pkgInfo(olsrr)
library(olsrr)
library(olsrr)
remove.packages("olsrr", lib="~/R/win-library/3.6")
install.packages("olsrr")
library(olsrr)
library(olsrr)
full.model <- lm(bmi ~ PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f +
PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa, data = df2)
# Stepwise regression model
step.modelb <- stepAIC(full.model, direction = "backward",
trace = FALSE)
df2 <-  no_out
lista <- c("id","visitdate","heightcm","weightkg","vdate","heightm","zbmius","zbmicatus","birthdate","bdate")
df2[,lista] <- list(NULL)
df2
# Fit the full model
full.model <- lm(bmi ~ PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f +
PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa, data = df2)
# Stepwise regression model
step.modelb <- stepAIC(full.model, direction = "backward",
trace = FALSE)
library(tidyverse)
library(caret)
library(leaps)
library(MASS)
df2 <-  no_out
lista <- c("id","visitdate","heightcm","weightkg","vdate","heightm","zbmius","zbmicatus","birthdate","bdate")
df2[,lista] <- list(NULL)
df2
# Fit the full model
full.model <- lm(bmi ~ PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f +
PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa, data = df2)
# Stepwise regression model
step.modelb <- stepAIC(full.model, direction = "backward",
trace = FALSE)
step.modelf <- stepAIC(full.model, direction = "forward",
trace = FALSE)
step.modelb
step.modelf
summary(step.modelb)
summary(step.modelf)
lm(bmi ~ PC2_nut + PC4_nut + age + PC5_taxa, df2)
lm_final <- lm(bmi ~ PC2_nut + PC4_nut + age + PC5_taxa, df2)
lm_final
summaru(lm_final)
summary(lm_final)
step.modelB
step.modelb <- stepAIC(full.model, direction = "backward",
trace = FALSE)
step.modelf <- stepAIC(full.model, direction = "forward",
trace = FALSE)
step.modelb
step.modelf
summary(step.modelb)
summary(step.modelf)
library(ggplot2)
library(ggplot2)
library(caret)
remove.packages("caret", lib="~/R/win-library/3.6")
library(ggplot2)
install.packages("ggplot2", dependencies=TRUE)
library(ggplot2)
install.packages("ggplot2")
library(ggplot2)
library("ggplot2")
library(ggplot2)
library(ggplot2)
library(dplyr)
library(ggplot2)
remove.packages("ggplot2")
install.packages("ggplot2")
library(ggplot2)
update.packages(repos='http://cran.rstudio.com/', ask=FALSE, checkBuilt=TRUE)
library(ggplot2)
install.packages("ggplot2")
update.packages(ggplot2)
update.packages("ggplot2")
library(ggplot2)
