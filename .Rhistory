curve(dnorm(x, mean=mean(z), sd=sd(z)),
add=TRUE, col="red", lwd=2)
lines(density(z)$x, density(z)$y,
col="black", lwd=2, lty = 2)
legend("topright",
legend = c( "Normal Curve", "Kernel Density Curve"),
lty=1:2, col=c("red","black"), cex=.7)
}
#DensitÃ  + Forma
yplot(df$bmi)
yplot(log(df$bmi))
#Analisi per sesso:
ggplot(no_out, aes(x = bmi, fill = as.factor(sex1m2f))) +
geom_density(size = 0.6, alpha = .3, colour = "black") +
geom_rug(aes(x = bmi,y = 0), position = position_jitter(height = 0)) +
labs(x = "Bmi", y =
"Densita'", fill = "Sesso") +
scale_fill_manual(label = c("Maschio","Femmina"), values = c("green","lightgrey"))+
ggtitle("Distribuzione del bmi rispetto al sesso")
```
```{r}
qqnorm(df$bmi, pch = 1, frame = FALSE, col = "tomato")
qqline(df$bmi, col = "steelblue", lwd = 2)
qqnorm(log(df$bmi), pch = 1, frame = FALSE, col = "tomato") #sembrerebbe che il log permetta di schiacciare la coda
qqline(log(df$bmi), col = "steelblue", lwd = 2)
```
Presenta una forma leptocurtica con un asimmetria positiva
Test non parametrici:
```{r}
shapiro.test(df$bmi) #H0: normalitÃ , si rigetta l'ipotesi nulla
shapiro.test(log(df$bmi)) #Il valore W si avvicina di piÃ¹ a 1
```
```{r}
plot(df$bmi)
abline(h=36, col="blue")
abline(h=10, col = "blue")
text(df$bmi, labels=df$id, cex= 0.7)
```
```{r}
no_out <- df
id_out <- c(4018,4009,3056,3046,3022,3018)
#no_out <- df[-c(15,52,83,88,48),]
no_out <- no_out[!(no_out$id %in% id_out),]
```
```{r}
qqnorm(no_out$bmi, pch = 1, frame = FALSE, col = "tomato")
qqline(no_out$bmi, col = "steelblue", lwd = 2)
qqnorm(log(no_out$bmi), pch = 1, frame = FALSE, col = "tomato")
qqline(log(no_out$bmi), col = "steelblue", lwd = 2)
```
```{r}
shapiro.test(no_out$bmi) #H0: normalitÃ , si accetta l'ipotesi di normalitÃ  dopo aver eliminato 7 outliers ()
shapiro.test(log(no_out$bmi)) #In questo caso il log peggiora
```
ggplot(data = df)+
geom_boxplot(aes(y = bmi), fill = "gray")  +
theme_update() +
labs(title = "Identificazione outliers mediante boxplot")
--------------------------------------------
Continuo analisi esplorativa delle variabili
--------------------------------------------
#Testiamo se esiste una differenza significativa nelle due popolazioni maschili e femminili:
```{r}
wilcox.test(df$bmi~df$sex1m2f,  conf.int = T, paired = F) #non esiste una differenza significativa tra le distribuzioni delle due popolazioni legate al sesso. Si accetta l'ipotesi nulla ad un livello di confidenza del 95%.
t.test(no_out$bmi~no_out$sex1m2f)
```
aggregate(no_out$bmi, list(no_out$sex1m2f), sd)
```{r}
#Analisi per etÃ :
no_out$age <- as.integer(no_out$age)
no_out$age_discr <- quantcut(no_out$age,3) #quantile discretization in 3 bins
ggplot(no_out, aes(x = bmi, fill = age_discr)) +
geom_density(size = 0.6, alpha = .3, colour = "black") +
geom_rug(aes(x = bmi,y = 0), position = position_jitter(height = 0)) +
labs(x = "Bmi", y =
"Densita'", fill = "Eta' discretizzata") +
ggtitle("Come si distribuisce il bmi rispetto all'eta'")
```
Chi-squared test tra fasce di etÃ  e bmi:
```{r}
no_out$bmi_discr <- quantcut(no_out$bmi,3)
age_bmi <- table(no_out$bmi_discr, no_out$age_discr)
chisq.test(age_bmi)
#Dal test del chi-quadro emerge che esiste una dipendenza tra le varie fasce di etÃ  e il bmi
```
```{r}
options(scipen=999)
df$bdate <- NULL
df$zbmius <- NULL
df$zbmicatus <- NULL
df$bmicat1norm2ow3ob <- NULL
no_out$cluster <- as.character(no_out$cluster)
no_out$cluster[no_out$cluster == "Bacteroides"] = 2
no_out$cluster[no_out$cluster == "Prevotella"] = 1
no_out$cluster <- as.numeric(no_out$cluster)
#Analisi pca nutrienti, var. demografiche, bmi
var_num <- c("bmi", "PC1_nut","PC2_nut","PC3_nut","PC4_nut","PC5_nut","age","sex1m2f", "cluster")
cor(no_out[,var_num])
plot(no_out[,var_num],cex=.8, col = "tomato")
var_num <- c("bmi", "PC1_taxa","PC2_taxa","PC3_taxa","PC4_taxa","PC5_taxa","PC6_taxa","age", "PC1_nut","PC2_nut","PC3_nut","PC4_nut","PC5_nut",
"sex1m2f")
library(corrplot)
png(height=1200, width=1500, pointsize=25,file="immagini/corr_plot.png")
coor <- cor(no_out[,var_num], method = "spearman")
corr_plot <- corrplot(coor, method="color", addCoef.col="black", order = "AOE", title = "Analisi correlazione di Spearman",
mar=c(1,1,1,1))
dev.off()
```
------------
Modellazione
------------
```{r}
#Train e test split:
# Random sample indexes
set.seed(20)
train_index <- sample(1:nrow(no_out), 0.9 * nrow(no_out))
test_index <- setdiff(1:nrow(no_out), train_index)
# Build X_train, y_train, X_test, y_test
X_train <- no_out[train_index, -11]
y_train <- no_out[train_index, "bmi"]
X_test <- no_out[test_index, -11]
y_test <- no_out[test_index, "bmi"]
```
```{r}
get_r2 <- function(test.y, test.pred){
SS.total      <- sum((test.y - mean(test.y))^2)
SS.residual   <- sum((test.y - test.pred)^2)
SS.regression <- sum((test.pred - mean(test.y))^2)
SS.total <- (SS.regression+SS.residual)
r2 <- SS.regression/SS.total
return(r2)
}
```
Modello 1: PCA taxa + variabili demografiche + clustering
```{r}
mod_lin1 <- lm(y_train ~  PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa + age + sex1m2f + cluster, X_train)
summary(mod_lin1)
anova(mod_lin1)
#CV:
train_data <- cbind(y_train, X_train)
train.control <- trainControl(method = "cv", number = 10,savePred=T)
model1 <- train(y_train ~ PC1_taxa + PC2_taxa + PC3_taxa + PC4_taxa + PC5_taxa + PC6_taxa + age + sex1m2f, train_data,
method = "lm", trControl = train.control)
get_confid_i <- function(model1, model_type = "Mod1"){
ci_mod1 <- as.data.frame(model1$resample$RMSE)
colnames(ci_mod1)[1] <- "RMSE"
#ci_mod1$type <- "Model 1"
media <- mean(ci_mod1$RMSE)
st_dev <- sd(ci_mod1$RMSE)
stand_error <- (1/sqrt(10))*st_dev
lower_bound <- media - 1.96*stand_error
upper_bound <- media + 1.96*stand_error
get_df <- as.data.frame(cbind(media, lower_bound, upper_bound))
get_df$type <- model_type
return(get_df)
}
info_mod1 <- get_confid_i(model1)
Metrics::mape(predict(model1,X_test), y_test)
```
Modello 2: PCA nutrienti + variabili demografiche + clustering:
```{r}
mod_lin2 <- lm(y_train ~  PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f + cluster, X_train)
summary(mod_lin2)
anova(mod_lin2)
#CV:
train_data <- cbind(y_train, X_train)
train.control <- trainControl(method = "cv", number = 10,savePred=T)
model2 <- train(y_train ~ PC1_nut + PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f, train_data,
method = "lm", trControl = train.control)
model2$resample
Metrics::rmse(predict(model2,X_test), y_test)
get_r2(predict(model2,X_test), y_test)
info_mod2 <- get_confid_i(model2, model_type = "Mod2")
```
# Test per omoschedasticitÃ :
```{r}
white.test <- function(lmod,data=no_out){
u2 <- lmod$residuals^2
y <- fitted(lmod)
Ru2 <- summary(lm(u2 ~ y + I(y^2)))$r.squared
LM <- nrow(data)*Ru2
p.value <- 1-pchisq(LM, 2)
data.frame("Test statistic"=LM,"P value"=p.value)
}
white.test(mod_lin2)
```
#Si accetta l'assunzione di omoschedasticitÃ 
-------------------------
VERIFICO AUTOCORRELAZIONE
-------------------------
1)Distribuzione dei residui
```{r}
plot(1:nrow(X_train),resid(mod_lin2),xlab="Observation Index",ylab="Residui",pch=19) #Graficamente non si vede bene la correlazine dei residui. E' piu utile in questo caso il test di dwatson
abline(h=0,col=2,lwd=3,lty=2)
```
Test di darbin-watson
```{r}
library(pander)
pander(dwtest(mod_lin2),big.mark=",") #essendo la statistica d compresa tra 1 e 3 possiamo affermare che non c'? autocorrelazione seriale di primo ordine tra i residui (ricorda che per d<1 c'? autocorrelazione positiva mentre per d>3 c'? autocorrelazione negativa) In ogni caso non mi aspetto un'autocorrelazione dei residui in quanto non ? una serie temporale (e anche se ci fosse autocorrelazione potrei decidere di non correggerla proprio perch? non ? una serie temporale)
```
---------------------
NormalitÃ  dei residui
---------------------
Test della Kurtosis
```{r}
library(normtest)
kurtosis.norm.test(mod_lin2$residuals, nrepl=2000)  #H0 normalit??, p-value < livello alpha: non norm.
#In questo caso si rifiuta l'ipotesi di normalit??
```
Test di shapiro wilk
```{r}
shapiro.test(mod_lin2$residuals) #Ipotesi di normalit? rifiutata
```
```{r}
stdres <- rstandard(mod_lin2) #residui standardizzati
probDist <- pnorm(stdres) #probabilit?? dei residui standardizzati
plot(ppoints(length(stdres)), sort(probDist), main = "PP-Plot", xlab = "Observed Probability", ylab = "Expected Probability")
abline(0,1)
```
Outliers sui residui
```{r}
library(olsrr)
olsrr::ols_plot_cooksd_chart(mod_lin2)
```
```{r}
# Stepwise regression:
library(tidyverse)
library(caret)
library(leaps)
library(MASS)
new_df <- read.csv("datasets/genera_all_vars_df.csv")
id_out <- c(4018,4009,3056,3046,3022,3018)
new_df$age <- as.integer(new_df$age)
#new_df <- new_df
new_df <- new_df[!(new_df$id %in% id_out),]
lista <- c("id","visitdate","heightcm","vdate","heightm","zbmius","zbmicatus","birthdate","bdate","bmicat1norm2ow3ob","weightkg",
"PC1_taxa","PC2_taxa")
new_df[,lista] <- list(NULL)
new_df2 <- new_df
new_df <- new_df[,-c(4:108)]
#Train e test split:
# Random sample indexes
#set.seed(20)
#train_index <- sample(1:nrow(new_df), 0.9 * nrow(new_df))
#test_index <- setdiff(1:nrow(new_df), train_index)
# Build X_train, y_train, X_test, y_test
X_train <- new_df[train_index, -3]
y_train <- new_df[train_index, "bmi"]
X_test <- new_df[test_index, -3]
y_test <- new_df[test_index, "bmi"]
train_data <- cbind(y_train,X_train)
# Fit the full model
full.model <- lm(y_train  ~. , data = X_train)
#full.model <- lm(bmi ~ PC1_nut+PC2_nut + PC3_nut + PC4_nut + PC5_nut + age + sex1m2f +
#                  PC1_taxa+PC2_taxa, data = new_df)
summary(full.model)
# Stepwise regression model
step.modelb <- stepAIC(full.model, direction = "backward",
trace = FALSE)
step.modelf <- stepAIC(full.model, direction = "forward",
trace = FALSE)
step.modelb
step.modelf
summary(step.modelb)
summary(step.modelf)
#Oppure:
# Train the model
train.control <- trainControl(method = "cv", repeats = 1, number = 9,savePred=T) #procedura di crossvalidation, number = 9, repeats = 1
step.modelB <- train(y_train ~., data = train_data,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:45), #da 1 a 3 variabili
trControl = train.control
)
trellis.par.set(caretTheme())
ggplot(step.modelB, metric = "MAE")
ggplot(step.modelB, metric = "Rsquared")
ggplot(step.modelB, metric = "RMSE")
step.modelB$finalModel
step.modelB$results
coef(step.modelB$finalModel, as.double(step.modelB$bestTune))
step.modelB$bestTune
summary(step.modelB$finalModel)
step.modelB$results
#Di nuovo stepwise regression nel subset scelto:
train.control <- trainControl(method = "cv", repeats = 1, number = 9,savePred=T,
returnResamp = "all") #procedura di crossvalidation, number = 9, repeats = 1
step.modelB <- train(y_train ~., data = train_data,
method = "leapBackward",
tuneGrid = data.frame(nvmax = 1:45), #da 1 a 3 variabili
trControl = train.control
)
step.modelB$finalModel
ggplot(step.modelB, metric = "RMSE")
resempla_cv <- step.modelB$resample
mean_agg <- aggregate(resempla_cv$RMSE, list(resempla_cv$nvmax), mean)
std_cv <- aggregate(resempla_cv$RMSE, list(resempla_cv$nvmax), sd)
create_df <- function(x, met = "std"){
colnames(x)[1] <- "Regressor_#"
colnames(x)[2] <- met
return(x)
}
std_cv <- create_df(std_cv, met = "std")
mean_cv <- create_df(mean_agg, met = "mean")
mn_sd_cv <- cbind(mean_cv, std_cv)
mn_sd_cv <- mn_sd_cv[,-3]
mn_sd_cv$std_error <- 1/(sqrt(10))*mn_sd_cv$std
mn_sd_cv$lower_bound <- mn_sd_cv$mean - 1.96 *mn_sd_cv$std_error
mn_sd_cv$upper_bound <- mn_sd_cv$mean + 1.96 *mn_sd_cv$std_error
res_r2 <- step.modelB$results$Rsquared
mn_sd_cv$rsquared <- res_r2
ci_stepwise_plot <- ggplot(mn_sd_cv) +
geom_point(data = mn_sd_cv, aes(x =`Regressor_#`,y = mean,color = "#000099"), alpha = 0.5) +
geom_errorbar(data = mn_sd_cv,aes(x = `Regressor_#`,y = mean, ymin = lower_bound, ymax = upper_bound,color = "#000099"), alpha = 0.5, width=.3,position=position_dodge(0.1))+
geom_line(aes(x = `Regressor_#`,y = rsquared*10,color = "green")) +
geom_point(aes(x = `Regressor_#`,y = rsquared*10,color = "green")) +
scale_y_continuous(sec.axis = sec_axis(trans = ~./10,name = "R-squared")) +
labs(title="Stepwise Backward regression (CV 10 folds)",
subtitle="Intervallo di confidenza CV rispetto al RMSE, R-squared (valor medio)",
y = "RMSE (mean cv)",
x = "Numero di regressori",
color = "Metrica")+
scale_color_manual(values=c("blue", "black"), labels = c("CI 95% RMSE", "R-squared"))
#ggsave("ci_stepwise_backward.png", plot = ci_stepwise_plot, height=8.5, width=15.57)
```
#CV SU MODELLO SCELTO DALLA STEPWISE
```{r}
#Best model in stepwise cv forward:
train.control <- trainControl(method = "cv", repeats = 1, number = 9,savePred=T) #procedura di crossvalidation, number = 9, repeats = 1
model<- train(y_train ~ sex1m2f + age + k__Bacteria.p__Bacteroidetes.c__Bacteroidia.o__Bacteroidales.f__Odoribacteraceae.g__Butyricimonas +
k__Bacteria.p__Firmicutes.c__Clostridia.o__Clostridiales.f__Lachnospiraceae.g__Lachnobacterium +
k__Bacteria.p__Firmicutes.c__Clostridia.o__Clostridiales.f__Ruminococcaceae.g__Oscillospira +
k__Bacteria.p__Firmicutes.c__Clostridia.o__Clostridiales.f__Veillonellaceae.g__Megasphaera +
PC5_nut,data = train_data, method = "lm",
#tuneGrid = data.frame(nvmax = 1:10), #da 1 a 3 variabili
trControl = train.control)
info_mod3 <- get_confid_i(model, model_type = "Mod3")
final_info <- rbind(info_mod1,info_mod2, info_mod3)
final_info$Rsquaredadj <- c(0.3551, 0.3446,0.5113)
```
--------------
Modello Finale
--------------
```{r}
final_model <-lm(y_train ~  sex1m2f + age + k__Bacteria.p__Bacteroidetes.c__Bacteroidia.o__Bacteroidales.f__Odoribacteraceae.g__Butyricimonas +
k__Bacteria.p__Firmicutes.c__Clostridia.o__Clostridiales.f__Lachnospiraceae.g__Lachnobacterium +
k__Bacteria.p__Firmicutes.c__Clostridia.o__Clostridiales.f__Ruminococcaceae.g__Oscillospira +
k__Bacteria.p__Firmicutes.c__Clostridia.o__Clostridiales.f__Veillonellaceae.g__Megasphaera +
PC5_nut,data = train_data) #mape 0.07
summary(final_model)
shapiro.test(final_model$residuals)
white.test(final_model)
qqnorm(final_model$residuals, pch = 1, frame = FALSE, col = "tomato")
qqline(final_model$residuals, col = "steelblue", lwd = 2)
ols_vif_tol(final_model) #VIF minore di 10 non sussiste collinearitÃ  delle variabili
#Predizioni:
predictions <- predict(final_model, X_test)
Metrics::rmse(y_test,predictions) #3.44
get_r2(y_test, predictions)
#Plot hyperplane
rockchalk::plotPlane(final_model, plotx1 = "sex1m2f", plotx2 = "age",ticktype = "detailed",npp = 10, theta = 30)
```
------
LASSO
------
```{r}
train.control <- trainControl(method = "cv",
number = 10, savePredictions = "final", returnResamp = 'all')
print_lasso <- function(dataset) {
# split in train and test: 90% obs in train and 10% in test
training_set = dataset[train_index,]
test_set = dataset[-train_index,]
#lambdas = tsutils::lambdaseq(training_set[,-1], training_set[,1], weight = NA, alpha = 1, standardise = TRUE,
#                         lambdaRatio = 1e-04, nLambda = 100, addZeroLambda = FALSE)$lambda
lambdas = seq(0.1,2, 0.025)
# Train the model
model_lasso <- train(bmi ~., data = training_set, method = "glmnet", preProcess="scale",
trControl = train.control, tuneGrid=expand.grid(alpha=1, lambda=lambdas))
#print(model_lasso)
#print(coef(model_lasso$finalModel,model_lasso$finalModel$lambdaOpt))
mape = 100*mean(abs((test_set[,1] - as.numeric(predict(model_lasso, test_set)))/test_set[,1]))
mse = mean((test_set[,1] - as.numeric(predict(model_lasso, test_set)))**2)
resempla_cv <- model_lasso$resempla_cv <- model_lasso$resample
mean_agg <- aggregate(resempla_cv$RMSE, list(resempla_cv$lambda), mean)
std_cv <- aggregate(resempla_cv$RMSE, list(resempla_cv$lambda), sd)
std_cv <- create_df(std_cv, met = "std")
mean_cv <- create_df(mean_agg, met = "mean")
mn_sd_cv <- cbind(mean_cv, std_cv)
mn_sd_cv <- mn_sd_cv[,-3]
mn_sd_cv$std_error <- 1/(sqrt(10))*mn_sd_cv$std
mn_sd_cv$lower_bound <- mn_sd_cv$mean - 1.96 *mn_sd_cv$std_error
mn_sd_cv$upper_bound <- mn_sd_cv$mean + 1.96 *mn_sd_cv$std_error
res_r2 <- model_lasso$results$Rsquared
print(res_r2)
mn_sd_cv$rsquared <- res_r2
ci_stepwise_plot <- ggplot(mn_sd_cv) +
geom_point(data = mn_sd_cv, aes(x =`Regressor_#`,y = mean,color = "#000099"), alpha = 0.5) +
geom_errorbar(data = mn_sd_cv,aes(x = `Regressor_#`,y = mean, ymin = lower_bound, ymax = upper_bound,color = "#000099"), alpha = 0.5, width=.3)+
geom_line(aes(x = `Regressor_#`,y = rsquared*10,color = "green")) +
geom_point(aes(x = `Regressor_#`,y = rsquared*10,color = "green")) +
scale_y_continuous(sec.axis = sec_axis(trans = ~./10,name = "R-squared")) +
labs(title="Lasso regression (CV 10 folds), selezione lambda migliore ",
subtitle="Intervallo di confidenza CV rispetto al RMSE, R-squared (valor medio)",
y = "RMSE (mean cv)",
x = "Lambda",
color = "Metrica")+
scale_color_manual(values=c("blue", "black"), labels = c("CI 95% RMSE", "R-squared"))
#lambdas_rmse = demo_nutrients_cluster_model$resample$RMSE
#mean_lambdas_rmse = mean(model_lasso$results$RMSE)
#sd_lambdas_rmse = sd(model_lasso$results$RMSE)
#list = list()
#list[["mape"]] = mape
#list[["mse"]] = mse
#list[["rmse"]] = mse**(1/2)
#list[["rmse_lower"]] = mn_sd_cv$lower_bound
#list[["rmse_upper"]] = mn_sd_cv$upper_bound
#print(list)
return(list(ci_stepwise_plot, model_lasso,training_set,mn_sd_cv))
}
create_df <- function(x, met = "std"){
colnames(x)[1] <- "Regressor_#"
colnames(x)[2] <- met
return(x)
}
dataset <- read.csv("datasets/demo_nutrients_cluster.csv")
dataset$X <- NULL
lasso_plt <- print_lasso(dataset)
lasso_plt[[2]]$bestTune
#ggsave("lasso_demo_nutriens_cluster.png", lasso_plt[[1]], height=8.5, width=15.57)
get_lasso <- function(lasso_plt, lamb = 0.6, typ = "lasso3"){
train.control <- trainControl(method = "cv", repeats = 1, number = 10,savePred=T,
returnResamp = "all") #procedura di crossvalidation, number = 9, repeats = 1
model<- train(bmi ~.,data = lasso_plt[[3]], method = "glmnet", tuneGrid=expand.grid(alpha=1, lambda=lamb),
trControl = train.control)
info_mod <- get_confid_i(model, model_type = typ)
return(list(info_mod, model))
}
lass1 <- get_lasso(lasso_plt, lamb = 0.6, typ = "lasso1")
info_mod4 <- lass1[[1]]
info_mod4$Rsquaredadj <-lass1[[2]]$results$Rsquared
final_info <- rbind(final_info, info_mod4)
new_df$cluster <- NULL
str(new_df)
lasso_plt <- print_lasso(new_df)
lasso_plt[[4]]
ggsave("lasso_demo_37taxa_cluster.png", lasso_plt[[1]], height=8.5, width=15.57)
lass2 <- get_lasso(lasso_plt,lamb = 0.8, typ = "lasso2")
info_mod5 <- lass2[[1]]
info_mod5$Rsquaredadj <-lass2[[2]]$results$Rsquared
final_info <- rbind(final_info, info_mod5)
new_df2 <- new_df2[,-c(147:151)]
new_df2 <- new_df2[,-ncol(new_df2)]
lasso_plt <- print_lasso(new_df2)
#ggsave("lasso_demo_37taxa_nutriens_cluster.png", lasso_plt[[1]], height=8.5, width=15.57)
lasso_plt[[2]]$bestTune
lass3 <- get_lasso(lasso_plt,lamb = 0.85, typ = "lasso3")
info_mod6 <- lass3[[1]]
info_mod6$Rsquaredadj <-lass3[[2]]$results$Rsquared
final_info <- rbind(final_info, info_mod6)
```
```{r}
final_info$type[final_info$type == "Mod1"] = "Mod.lineare 1"
final_info$type[final_info$type == "Mod2"] = "Mod.lineare 2"
final_info$type[final_info$type == "Mod3"] = "Mod.stepwise"
final_info$type[final_info$type == "lasso1"] = "Lasso 1"
final_info$type[final_info$type == "lasso2"] = "Lasso 2"
final_info$type[final_info$type == "lasso3"] = "Lasso 3"
mod_las_lin_plot <- ggplot(final_info)  +
geom_errorbar(aes(x = type,y = media, ymin = lower_bound, ymax = upper_bound,color = "#000099"), alpha = 0.5, width=.3,position=position_dodge(0.1)) +
geom_point(aes(x = type,y = media,,color = "#000099"), alpha = 0.5) +
geom_line(aes(x = type, y = Rsquaredadj*10, color = "black"),group = 1) +
geom_point(aes(x = type, y = Rsquaredadj*10, color = "black"),group = 1) +
scale_y_continuous(sec.axis = sec_axis(trans = ~./10,name = "R-squared adj"))+
scale_color_manual(values=c("blue", "black"), labels = c("CI 95% RMSE", "R-squared adj")) +
labs(title="Intervallo di confidenza CV rispetto al RMSE",
subtitle="Mettiamo a confronto gli errori ottenuti in cv dai diversi modelli e R-squared adj",
y = "RMSE (mean cv)",
x = "Tipologia di modello",
color = "Metriche")
#ggsave("Plot/confronto_tutti_i_modelli.png", mod_las_lin_plot, height=8.5, width=15.57)
```
```{r}
training_set = dataset[train_index,]
test_set = dataset[-train_index,]
mod_lasso_1 <- glmnet::glmnet(as.matrix(training_set[,-1]), as.matrix(training_set[,1]), alpha = 1, lambda = 0.43)
pred_lasso1 <- predict(mod_lasso_1, s = 0.43, newx = as.matrix(test_set[,-1]))
Metrics::msre(test_set[,1],pred_lasso1) #3.645
get_r2(test_set[,1],pred_lasso1)
new_df$cluster <- as.character(new_df$cluster)
new_df$cluster[new_df$cluster == "Bacteroides"] = 1
new_df$cluster[new_df$cluster == "Prevotella"] = 2
new_df$cluster <- as.numeric(new_df$cluster)
mod_lasso_2 <- glmnet::glmnet(as.matrix(new_df[train_index,-3]), as.matrix(new_df[train_index,3]), alpha = 1, lambda = 0.6)
pred_lasso2 <- predict(mod_lasso_2, s = 0.6, newx = as.matrix(new_df[test_index,-3]))
Metrics::rmse(new_df[test_index,3],pred_lasso2) #3.015
get_r2(new_df[test_index,3],pred_lasso2)
mod_lasso_3 <- glmnet::glmnet(as.matrix(new_df2[train_index,-3]), as.matrix(new_df2[train_index,3]), alpha = 1, lambda = 0.85)
pred_lasso3 <- predict(mod_lasso_3, s = 0.85, newx = as.matrix(new_df2[test_index,-3]))
Metrics::rmse(new_df2[test_index,3],pred_lasso3) #3.006
mod_lasso_1
print(coef(mod_lasso_1$finalModel,mod_lasso_1$finalModel$lambdaOpt))
lass1
lass1
lasso_plt <- print_lasso(dataset)
lasso_plt[[2]]$bestTune
lasso_plt[[2]]$bestTune
coef(lasso_plt[[2]]$finalModel)
print(coef(lasso_plt[[2]]$finalModel,lasso_plt[[2]]$finalModel$lambdaOpt))
lasso_plt <- print_lasso(new_df)
lasso_plt <- print_lasso(new_df)
print(coef(lasso_plt[[2]]$finalModel,lasso_plt[[2]]$finalModel$lambdaOpt))
lasso_plt <- print_lasso(new_df2)
print(coef(lasso_plt[[2]]$finalModel,lasso_plt[[2]]$finalModel$lambdaOpt))
